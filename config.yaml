data:
  raw_data_path: "data/raw_files"
  processed_data_path: "data/preprocessed/corpus.txt"
  tokenizer_save_path: "artifacts/custom_tokenizer"

model:
  block_size: 128
  n_embd: 256
  n_head: 4
  n_layer: 4
  dropout: 0.1

training:
  batch_size: 32
  epochs: 1
  optimizer: "AdamW"
  learning_rate: 0.0001
  reports_per_epoch: 10
  early_stopping: False
  load_recent: False
  patience: 5
  split_ratio: 0.9
  state_save_path: "artifacts/runs"

generation:
  file: "output.txt"
  chunksize: 20

pipeline:
  step_1: "data_ingestion"
  step_2: "tokenization"
  step_3: "model_training"
  step+4: "evaluation"
